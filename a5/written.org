#+latex_class_options: [10pt]
#+LATEX_HEADER: \usepackage[margin=1.25in]{geometry}


*Assignment 5 - Written:* Sub-Word Modeling Neural Machine Translation \\

* 1. Character-based convolutional encoder for NMT

*(a)* In Assignment 4 we used 256-dimensional word embeddings ($e_{word}$ = 256),
while in this assignment, it turns out that a character embedding size of 50 suffices ($e_{char} = 50$).
In 1-2 sentences, explain one reason why the embedding size used for character-level embeddings is
typically lower than that used for word embeddings.\\

@@latex:\noindent@@
*Solution:*
One reason the embedding size for character-level embeddings is lower is because the character space will most always be smaller than any word space. There's simply more words than characters so we are afforded smaller embedding spaces.\\


@@latex:\noindent@@
*(b)* Write down the total number of parameters in the character-based embedding
model (Figure 2), then do the same for the word-based lookup embedding model (Figure 1). Write
each answer as a single expression (though you may show working) in terms of $e_{char}$, k, $e_{word}$,
$V_{word}$ (the size of the word-vocabulary in the lookup embedding model) and $V_{char}$ (the size of the
character-vocabulary in the character-based embedding model).\\
 Given that in our code, $k = 5$, $V_{word} \approx 50,000$ and $V_{char} = 96$, state which model has more parameters, and by what factor (e.g. twice as many? a thousand times as many?)\\

@@latex:\noindent@@
*Solution:*

1. *character-based embedding*: For the embedding lookup layer we have a weight matrix of size
 $V_{char} \times e_{char}$. In the convolution layer portion we have a weight matrix of size $f \times e_{char} \times k$ and a
bias of size $f$. Note $f$ will be set to $e_{word}$ in our application. Lastly we take into account the
number of parameters in the projection and gate layers. Here we have project/gate matrices and biases
of size: $e_{word} \times e_{word}$ and $e_{word}$, respectively. So in all we'll have

#+BEGIN_LATEX
\begin{align*}
\text{total number of parameters} &= (V_{char} \times e_{char}) + ((f \times e_{char} \times k) + f) + ((e_{word} \times e_{word}) + e_{word})\\
&= (V_{char} \times e_{char}) + ((e_{word} \times e_{char} \times k) + e_{word}) + ((e_{word} \times e_{word}) + e_{word})\\
&= V_{char}e_{char} + ke_{word}e_{char} + e_{word} + e_{word}e_{word} + e_{word}\\
&= \bold{V_{char}e_{char} + ke_{word}e_{char} + e_{word}^2 + 2e_{word}}\\ \\
&= (96 * 50) + (5 * 256 * 50) + (256^2) + (2*256)\\
&= \bold{134,848}
\end{align*}
#+END_LATEX

2. *word-based lookup embedding*: Note that the word-based embedding model uses a single Embedding layer given a vocabulary and embedding vector size. So in all we'll have:

#+BEGIN_LATEX
\begin{align*}
\text{total number of parameters} &= \bold{V_{word} * e_{word}} \\ \\
&= (50,000 * 256)\\
&= \bold{2,800,000}
\end{align*}
#+END_LATEX

So in all the word embedding model has roughly 20 times more parameters
than the char-based one.\\

@@latex:\noindent@@
*(c)* In step 3 of the character-based embedding model, instead of using a 1D convnet, we could have used a RNN instead (e.g. feed the sequence of characters into a bidirectional
LSTM and combine the hidden states using max-pooling). Explain one advantage of using a convolutional architecture rather than a recurrent architecture for this purpose, making it clear how
the two contrast.\\

@@latex:\noindent@@
*Solution:* A 1D convnet allows us to naturally work with character level n-grams by constricting the
 receptive field (kernel) size of the convolution to $n$. This allows us to extract local morphemic
 features of a word that are not influenced by the characters outside of the field's current position
over said word. In contrast, an RNN can only visit characters of a word sequentially from
beginning-to-end and/or end-to-beginning. This suggests that hidden character embeddings outside of
a morphemic n-gram region can leak into the region's representation even though they will most
likely lack any substructural word affinity. Therefore building up word embeddings from character
sequences through an RNN may not embed the morpheme-like representation we'd enjoy with the 1D
conv.\\

@@latex:\noindent@@
*(d)* In lectures we learned about both max-pooling and average-pooling. For each
pooling method, please explain one advantage in comparison to the other pooling method. For
each advantage, make it clear how the two contrast, and write to a similar level of detail as in the
example given in the previous question.\\

@@latex:\noindent@@
*Solution:*\\

*max-pooling*: Max pooling compresses an input by extracting the most amplified activation signals in each windowed region of the input. By choosing the strongest signals, we ensure that the most prominent features in a region of a sequence get taken upstream as /the/ region's representative. The advantage is that the max signal will, in many cases, provide the best summary (most important feature) of its region and in turn produce better word representations that are locally invariant to changes. In contrast, average pooling does not always pass on the strongest possible signal. They can get "watered" down considerably by small neighboring activations (attack of the outliers!).\\

*average-pooling*: Average pooling compresses an input by extracting the average activation signals of each windowed region of the input. It has the advantage that it can take regions with a majority of large feature signals and output a smooth summary of the neighborhood so as to maintain some information provided by a minority of weak signals. On the other hand, max pooling would completely discard such rare signals, even if they are somewhat helpful in depicting the region.
